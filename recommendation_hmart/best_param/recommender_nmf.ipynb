{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|              userId|           productId|rating|\n",
      "+--------------------+--------------------+------+\n",
      "|65362c3195e84456c...|65363731f7a5f28d5...|     4|\n",
      "|65362c3195e84456c...|65363731262e9f91d...|     3|\n",
      "|65362c3195e84456c...|65363731c193c5580...|     1|\n",
      "|65362c3195e84456c...|653637316ac4c08ab...|     4|\n",
      "|65362c3195e84456c...|65363731f0f3e986c...|     1|\n",
      "|65362c3195e84456c...|653637315c9280c97...|     3|\n",
      "|65362c3195e84456c...|65363731403de5e9f...|     3|\n",
      "|65362c3195e84456c...|65363731a7165c457...|     5|\n",
      "|65362c3195e84456c...|653637313feea7faa...|     1|\n",
      "|65362c3195e84456c...|65363731e90db023f...|     5|\n",
      "|65362c3195e84456c...|6536373152560a326...|     5|\n",
      "|65362c3195e84456c...|65363731f022c5c7b...|     3|\n",
      "|65362c3195e84456c...|65363731a190e56d3...|     1|\n",
      "|65362c3195e84456c...|65363731a69e3e141...|     2|\n",
      "|65362c3195e84456c...|6536373107d8bbebf...|     3|\n",
      "|65362c3195e84456c...|65363731375072736...|     3|\n",
      "|65362c3195e84456c...|65363731a13ed99c0...|     4|\n",
      "|65362c3195e84456c...|6536373164e9465bd...|     2|\n",
      "|65362c3195e84456c...|653637317e0b60841...|     3|\n",
      "|65362c3195e84456c...|65363731d09ecf382...|     5|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "spark = SparkSession.builder.appName(\"NMF rec sys\").getOrCreate()\n",
    "product_rating = spark.read.csv(\"../data/Ratings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "product_rating.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product = spark.read.csv('../data/Products.csv', header = True)\n",
    "\n",
    "# Assuming you have a DataFrame named df and you want to rename column 'oldColumnName' to 'newColumnName'\n",
    "df_product = df_product.withColumnRenamed('id', 'productId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "|           productId|              userId|rating|               title|price|\n",
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "|65363731f7a5f28d5...|65362c3195e84456c...|     4| Lenovo ThinkPad 16P|  114|\n",
      "|65363731262e9f91d...|65362c3195e84456c...|     3|  Camera Sony ZV-E10|  600|\n",
      "|65363731c193c5580...|65362c3195e84456c...|     1|Apple Watch Serie...|  704|\n",
      "|653637316ac4c08ab...|65362c3195e84456c...|     4|Samsung Galaxy S2...|  236|\n",
      "|65363731f0f3e986c...|65362c3195e84456c...|     1|      HP Probook 440|  790|\n",
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df = product_rating.join(df_product, 'productId', 'inner')\n",
    "merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-----+-----+\n",
      "|productId|userId|rating|title|price|\n",
      "+---------+------+------+-----+-----+\n",
      "|        0|     0|     0|    0|    0|\n",
      "+---------+------+------+-----+-----+\n",
      "\n",
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "|           productId|              userId|rating|               title|price|\n",
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "|65363731f7a5f28d5...|65362c3195e84456c...|     4| Lenovo ThinkPad 16P|  114|\n",
      "|65363731262e9f91d...|65362c3195e84456c...|     3|  Camera Sony ZV-E10|  600|\n",
      "|65363731c193c5580...|65362c3195e84456c...|     1|Apple Watch Serie...|  704|\n",
      "|653637316ac4c08ab...|65362c3195e84456c...|     4|Samsung Galaxy S2...|  236|\n",
      "|65363731f0f3e986c...|65362c3195e84456c...|     1|      HP Probook 440|  790|\n",
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- productId: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check null for all cols:\n",
    "from pyspark.sql.functions import *\n",
    "merged_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in merged_df.columns]\n",
    "    ).show()\n",
    "\n",
    "merged_df.show(5)\n",
    "merged_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.2553\n",
      "RMSE: 2.2367\n",
      "RMSE: 1.6646\n",
      "RMSE: 1.4318\n",
      "Best Training Rate: 0.7999999999999999\n",
      "Best n_epochs: 11\n",
      "Best RMSE: 1.4318490764900942\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, accuracy, NMF\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def generate_sequential_list_float(start, end, increment):\n",
    "    sequential_list = list(np.arange(start, end, increment))\n",
    "    return sequential_list\n",
    "\n",
    "def generate_sequential_list(start, end, increment):\n",
    "    sequential_list = list(range(start, end, increment))\n",
    "    return sequential_list\n",
    "\n",
    "def calculate_rmse(train_value, k_values, merged_df):\n",
    "    df_train, df_test = merged_df.randomSplit([train_value, 1-train_value], seed=96)\n",
    "    df_train_pandas = df_train.toPandas()\n",
    "    df_test_pandas = df_test.toPandas()\n",
    "\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    data_train = Dataset.load_from_df(df_train_pandas[['userId', 'productId', 'rating']], reader)\n",
    "    data_test = Dataset.load_from_df(df_test_pandas[['userId', 'productId', 'rating']], reader)\n",
    "\n",
    "    trainset = data_train.build_full_trainset()\n",
    "    testset = data_test.build_full_trainset().build_testset()\n",
    "\n",
    "    rmse_values = []\n",
    "    for k in k_values:\n",
    "        algo = NMF(n_factors=15, n_epochs=k, biased=True)\n",
    "        algo.fit(trainset)\n",
    "        knn_predictions = algo.test(testset)\n",
    "\n",
    "        rmse = accuracy.rmse(knn_predictions)\n",
    "        rmse_values.append(rmse)\n",
    "\n",
    "    return rmse_values\n",
    "\n",
    "# ... (other setup code)\n",
    "\n",
    "# Specify the start, end, and increment for training rate\n",
    "transtart = 0.7\n",
    "tranend = 0.8\n",
    "incre = 0.1\n",
    "\n",
    "# Generate the sequential list for training rate\n",
    "train_values = generate_sequential_list_float(transtart, tranend, incre)\n",
    "\n",
    "# Specify the start, end, and increment for k values\n",
    "start_k = 1\n",
    "end_k = 20\n",
    "increment_k = 10\n",
    "\n",
    "# Generate the sequential list for k values\n",
    "k_values = generate_sequential_list(start_k, end_k, increment_k)\n",
    "\n",
    "# Initialize a 2D array to store RMSE values\n",
    "arr = np.zeros((len(train_values), len(k_values)))\n",
    "\n",
    "# Use 10 threads to parallelize the computation for different training rates\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    futures = [executor.submit(calculate_rmse, t, k_values, merged_df) for t in train_values]\n",
    "\n",
    "    for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "        rmse_values = future.result()\n",
    "        arr[i, :] = rmse_values\n",
    "\n",
    "# Find the indices of the minimum RMSE value in the array\n",
    "min_rmse_idx = np.unravel_index(np.argmin(arr), arr.shape)\n",
    "\n",
    "# Get the corresponding values\n",
    "best_train_value = train_values[min_rmse_idx[0]]\n",
    "best_k_value = k_values[min_rmse_idx[1]]\n",
    "best_rmse = arr[min_rmse_idx]\n",
    "\n",
    "print(\"Best Training Rate:\", best_train_value)\n",
    "print(\"Best n_epochs:\", best_k_value)\n",
    "print(\"Best RMSE:\", best_rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
